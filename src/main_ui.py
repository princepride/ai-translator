import zipfile
from typing import Optional, Tuple
from docx.opc.oxml import parse_xml
import yaml
import os
import shutil
import gradio as gr
from gradio.utils import NamedString
from utils.path import get_models
from utils.cuda import get_gpu_info
import json
import time
import importlib.util
from modules.file import FileReaderFactory, ExcelFileWriter
from docx.opc.pkgreader import _SerializedRelationships, _SerializedRelationship
from docx import Document
import markdown
from bs4 import BeautifulSoup
from docx.oxml.ns import qn
from docx.table import Table
from docx.text.paragraph import Paragraph
from docx.shared import Inches
from pptx import Presentation
import re
from transformers import AutoTokenizer
import openpyxl
from openai import OpenAI
import pandas as pd
import zhconv
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
load_dotenv()
# Ëé∑ÂèñÂΩìÂâçËÑöÊú¨ÊâÄÂú®ÁõÆÂΩïÁöÑÁªùÂØπË∑ØÂæÑ
script_dir = os.path.dirname(os.path.abspath(__file__))

# ÊûÑÂª∫baseConfig.ymlÂíåmodelExplains.ymlÁöÑÁªùÂØπË∑ØÂæÑ
file_path = os.path.join(script_dir, 'configs', 'baseConfig.yml')
tokenizer = AutoTokenizer.from_pretrained(os.path.join(script_dir, 'tokenzier'))

with open(file_path, 'r') as file:
    yaml_data = yaml.load(file, Loader=yaml.FullLoader)

available_gpus = get_gpu_info()
api_models = get_models(os.path.join(script_dir, 'models/API'))
local_models = get_models(os.path.join(script_dir, 'models/local'))
available_models = {**api_models, **local_models}
# available_languages = [] # Ê≥®ÈáäÊéâÊàñËÄÖÁßªÈô§ËøôË°åÔºåÂõ†‰∏∫ËØ≠Ë®ÄÈÄâÈ°πÁî± update_choices Âä®ÊÄÅÁîüÊàê

default_model_name = "gpt-4o-mini" # ÂÆö‰πâÈªòËÆ§Ê®°ÂûãÂêçÁß∞
default_original_language = "Chinese"  # ÂÆö‰πâÈªòËÆ§ÂéüÂßãËØ≠Ë®Ä
default_target_language_single = "English" # ÂÆö‰πâÈªòËÆ§ÁõÆÊ†áËØ≠Ë®Ä (ÂçïÈÄâ)
default_target_language_multi = ["English"] # ÂÆö‰πâÈªòËÆ§ÁõÆÊ†áËØ≠Ë®Ä (Â§öÈÄâ)

def update_row_selection(selected_value):
    if selected_value == "ÊâÄÊúâË°å":
        return gr.update(visible=False)
    else:
        return gr.update(visible=True)

# --- translate_excel, translate, translate_excel_folder, word_to_markdown, markdown_to_word, translate_markdown_folder, glossary_check ÂáΩÊï∞‰øùÊåÅ‰∏çÂèò ---
# (ÁúÅÁï•Ëøô‰∫õÂáΩÊï∞ÁöÑ‰ª£Á†Å‰ª•‰øùÊåÅÁÆÄÊ¥Å)
def translate_excel(input_file, start_row, end_row, start_column, target_column,
                    selected_model,
                    selected_lora_model, selected_gpu, batch_size, original_language, target_languages):
    start_time = time.time()
    file_path = input_file.name
    reader, fp = FileReaderFactory.create_reader(file_path)
    inputs = reader.extract_text(file_path, target_column, start_row, end_row)

    outputs = translate(inputs, selected_model, selected_lora_model, selected_gpu, batch_size, original_language,
                        target_languages)

    excel_writer = ExcelFileWriter()
    print("Finally processed number: ", len(outputs))
    output_file = excel_writer.write_text(file_path, outputs, start_column, start_row, end_row)

    end_time = time.time()
    return f"Total process time: {int(end_time - start_time)}s", output_file

def translate(inputs, selected_model, selected_lora_model, selected_gpu, batch_size, original_language, target_languages):
    if isinstance(inputs, str):
        inputs = [inputs]
    model_path = available_models.get(selected_model)
    if not model_path:
            print(f"Model '{selected_model}' not found in available models.")
            return [] # Return empty list or handle error appropriately

    model_file_path = os.path.join(model_path, 'model.py')
    # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
    if not os.path.exists(model_file_path):
        print(f"No model.py found in {model_path}")
        return [] # Return empty list or handle error appropriately
    spec = importlib.util.spec_from_file_location("model", model_file_path)
    if spec is None or spec.loader is None:
        print(f"Could not load spec for model.py in {model_path}")
        return []
    model_module = importlib.util.module_from_spec(spec)
    try:
        spec.loader.exec_module(model_module)
    except Exception as e:
        print(f"Error executing module {model_file_path}: {e}")
        return []
    outputs = []
    if hasattr(model_module, 'Model'):
        try:
            # Pass the base directory, not the model.py path itself
            model = model_module.Model(model_path, selected_lora_model, selected_gpu)
            if hasattr(model, 'generate'):
                outputs = model.generate(inputs, original_language, target_languages, batch_size)
            else:
                print("Model class does not have a 'generate' method.")
        except Exception as e:
            print(f"Error instantiating or running model from {model_path}: {e}")
    else:
        print("No Model class found in model.py.")
    return outputs

def translate_excel_folder(input_folder, start_row, end_row, start_column, target_column, selected_model,
                            selected_lora_model, selected_gpu, batch_size, original_language, target_languages,
                            row_selection):
    start_time = time.time()
    if not input_folder:
        return "No files uploaded", []

    folder_path = os.path.dirname(input_folder[0].name)
    processed_files = []

    # Create a new folder named 'processed' within the uploaded folder
    processed_folder = os.path.join(folder_path, 'processed')
    os.makedirs(processed_folder, exist_ok=True)

    for input_file in input_folder:
        file_path = input_file.name
        # Use factory to convert the file
        try:
            reader, updated_file_path = FileReaderFactory.create_reader(file_path)
        except ValueError as e:
            print(f"Error: {e}")
            continue

        # Ensure input_file refers to the updated file path if conversion occurred
        # This might be tricky with Gradio File objects, ensure .name is updated if needed
        original_file_obj_name = input_file.name # Store original name if needed
        if file_path != updated_file_path:
                # Be cautious modifying Gradio object properties directly if it causes issues
                # It might be better to work with updated_file_path directly
                # input_file.name = updated_file_path # Potentially risky
                file_path_to_process = updated_file_path
                # Create a NamedString or handle appropriately if translate_excel expects Gradio object
                temp_file_obj = NamedString(name=updated_file_path, data="", is_file=True) # Example adjustment
        else:
                file_path_to_process = file_path
                temp_file_obj = input_file # Use original object

        current_end_row = end_row
        if row_selection == "ÊâÄÊúâË°å":
            try:
                current_end_row = FileReaderFactory.count_rows(file_path_to_process)
            except Exception as e:
                print(f"Could not count rows for {file_path_to_process}: {e}. Skipping file or using default end_row.")
                continue # Or handle error differently

        try:
            # Pass the potentially modified file object/path
            process_time, output_file = translate_excel(temp_file_obj, start_row, current_end_row, start_column, target_column,
                                                        selected_model, selected_lora_model, selected_gpu, batch_size,
                                                        original_language, target_languages)

            # output_file is likely a path string returned by excel_writer
            if output_file and os.path.exists(output_file):
                    processed_file_path = os.path.join(processed_folder, os.path.basename(output_file))
                    shutil.move(output_file, processed_file_path)
                    processed_files.append(processed_file_path)
            else:
                    print(f"Translation failed or output file path invalid for {original_file_obj_name}")

        except Exception as e:
            print(f"Error processing file {original_file_obj_name}: {e}")
            continue # Skip to next file on error


    # Create a zip file
    zip_filename = os.path.join(folder_path, "processed_files.zip")
    if not processed_files:
            print("No files were processed successfully.")
            return "No files processed successfully.", None # Return None for zip file

    try:
        with zipfile.ZipFile(zip_filename, 'w') as zipf:
            for file in processed_files:
                if os.path.exists(file):
                        zipf.write(file, os.path.basename(file))
                        print(f"File {file} added to zip.")
                else:
                        print(f"Warning: Processed file {file} not found for zipping.")
    except Exception as e:
        print(f"Error creating zip file {zip_filename}: {e}")
        return f"Error creating zip file: {e}", None


    end_time = time.time()
    print(f"Total process time: {int(end_time - start_time)}s")
    print(f"Processed files added to zip: {processed_files}")
    return f"Total process time: {int(end_time - start_time)}s. {len(processed_files)} file(s) processed.", zip_filename
    

def word_to_markdown(docx_path, output_dir="images"):
    """
    Â∞ÜÊåáÂÆöÁöÑ .docx Êñá‰ª∂ËΩ¨Êç¢‰∏∫ Markdown Ê†ºÂºèÔºåÂπ∂ÊèêÂèñÂÖ∂‰∏≠ÁöÑÂõæÁâá„ÄÇ
    """
    # ÂàõÂª∫ËæìÂá∫ÂõæÁâáÁõÆÂΩï
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # --- Monkey Patching, ËøôÈÉ®ÂàÜÊòØÊ≠£Á°ÆÁöÑ, ‰øùÊåÅ‰∏çÂèò ---
    def load_from_xml_v2(baseURI, rels_item_xml):
        srels = _SerializedRelationships()
        if rels_item_xml is not None:
            try:
                rels_elm = parse_xml(rels_item_xml)
                for rel_elm in rels_elm.Relationship_lst:
                    if hasattr(rel_elm, 'target_ref') and rel_elm.target_ref not in ('../NULL', 'NULL', None):
                        srels._srels.append(_SerializedRelationship(baseURI, rel_elm))
            except Exception as e:
                print(f"Error parsing relationships XML: {e}")
        return srels
    _SerializedRelationships.load_from_xml = load_from_xml_v2
    # --- Monkey Patching ÁªìÊùü ---

    def iter_block_items(parent):
        if hasattr(parent, '_element') and hasattr(parent._element, 'body'):
            parent_elm = parent._element.body
            for child in parent_elm.iterchildren():
                if child.tag == qn('w:p'):
                    yield Paragraph(child, parent)
                elif child.tag == qn('w:tbl'):
                    yield Table(child, parent)

    try:
        doc = Document(docx_path)
    except Exception as e:
        print(f"Error opening document {docx_path}: {e}")
        return ""

    md_content = ""
    image_counter = 1
    image_paths_generated = []

    for block in iter_block_items(doc):
        if isinstance(block, Paragraph):
            para = block # para ÊòØ‰∏Ä‰∏™ Paragraph ÂØπË±°

            # --- Ê†∏ÂøÉ‰øÆÊ≠£ÁÇπÂú®ËøôÈáå ---
            # Ê£ÄÊü•ÊÆµËêΩ‰∏≠ÊòØÂê¶ÊúâÂõæÁâá„ÄÇxpath‰ºöËøîÂõû‰∏Ä‰∏™ÂàóË°®ÔºåÂ¶ÇÊûúÂàóË°®‰∏ç‰∏∫Á©∫ÂàôËØ¥ÊòéÊâæÂà∞‰∫Ü„ÄÇ
            # ÈîôËØØÂÜôÊ≥ïÔºöpara.element
            # Ê≠£Á°ÆÂÜôÊ≥ïÔºöpara._element
            if para._element.xpath('.//w:drawing'):
                # ÂêåÊ†∑ÔºåËøôÈáå‰πüÈúÄË¶Å‰ΩøÁî® _element
                for rId in para._element.xpath(".//a:blip/@r:embed"):
                    if rId and hasattr(doc.part, 'related_parts') and rId in doc.part.related_parts:
                        try:
                            image_part = doc.part.related_parts[rId]
                            image_bytes = image_part.blob
                            ext = os.path.splitext(image_part.partname)[-1].lower() or ".png"
                            if ext not in ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff']:
                                ext = ".png"
                            
                            image_name = f"image_{image_counter}{ext}"
                            image_path = os.path.join(output_dir, image_name)
                            
                            with open(image_path, 'wb') as f:
                                f.write(image_bytes)
                            
                            md_image_path = image_name
                            md_content += f"![{image_name}]({md_image_path})\n\n"
                            image_paths_generated.append(image_path)
                            image_counter += 1
                        except Exception as e:
                            print(f"Error processing image resource {rId}: {e}")
            
            # Â§ÑÁêÜÊÆµËêΩÊñáÊú¨
            para_text = para.text.strip()
            if para_text:
                if para.style and para.style.name.startswith('Heading'):
                    try:
                        level = int(para.style.name.split()[-1])
                        md_content += f"{'#' * level} {para_text}\n\n"
                    except (ValueError, IndexError):
                        md_content += f"### {para_text}\n\n"
                else:
                    md_content += f"{para_text}\n\n"

        elif isinstance(block, Table):
            table = block
            md_content += "\n"
            rows = table.rows
            if len(rows) > 0:
                header_cells = rows[0].cells
                header = "| " + " | ".join((cell.text or "").strip().replace('\n', ' ') for cell in header_cells) + " |\n"
                md_content += header
                md_content += "| " + " | ".join(['---'] * len(header_cells)) + " |\n"
                for row in rows[1:]:
                    row_cells = row.cells
                    row_text = "| " + " | ".join((cell.text or "").strip().replace('\n', ' ') for cell in row_cells) + " |\n"
                    md_content += row_text
            md_content += "\n"

    print(f"Generated images: {image_paths_generated}")
    return md_content
def markdown_to_word(md_content, word_path, image_base_dir="images"):
    md_content = md_content.replace('<', '&lt;').replace('>', '&gt;')
    try:
        html = markdown.markdown(md_content, extensions=['tables', 'fenced_code'])
    except Exception as e:
        print(f"Error converting Markdown to HTML: {e}")
        return

    try:
        soup = BeautifulSoup(html, 'html.parser')
    except Exception as e:
        print(f"Error parsing generated HTML: {e}")
        return

    doc = Document()

    for element in soup.children:
        if not hasattr(element, 'name'):
            text = str(element).strip()
            if text:
                doc.add_paragraph(text)
            continue

        if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            try:
                level = int(element.name[1])
                doc.add_heading(element.get_text(strip=True), level=level)
            except (ValueError, IndexError):
                doc.add_heading(element.get_text(strip=True), level=3)
        
        elif element.name == 'p':
            # Â¶ÇÊûúÊÆµËêΩÂÜÖÂÆπÊòØÂõæÁâáÔºåÂàôÁâπÊÆäÂ§ÑÁêÜ
            if element.find('img'):
                img_tag = element.find('img')
                img_src = img_tag.get('src')
                alt_text = img_tag.get('alt', '')
                img_path = os.path.join(image_base_dir, img_src)
                if os.path.exists(img_path):
                    try:
                        doc.add_picture(img_path, width=Inches(5.5))
                    except Exception as e:
                        print(f"Warning: Could not add picture {img_path}. Error: {e}")
                        doc.add_paragraph(f"[Image: {alt_text or img_src}]")
                else:
                    print(f"Warning: Could not find image file at a constructed path: {img_path} (source was: {img_src})")
                    doc.add_paragraph(f"[Image not found: {alt_text or img_src}]")
            # Âê¶Âàô‰Ωú‰∏∫ÊôÆÈÄöÊÆµËêΩÂ§ÑÁêÜ
            else:
                text = element.get_text()
                if text.strip():
                    doc.add_paragraph(text)
        
        elif element.name == 'ul':
            for li in element.find_all('li', recursive=False):
                doc.add_paragraph(li.get_text(strip=True), style='List Bullet')
        elif element.name == 'ol':
            for li in element.find_all('li', recursive=False):
                doc.add_paragraph(li.get_text(strip=True), style='List Number')
        elif element.name == 'pre' and element.find('code'):
            code_text = element.find('code').get_text()
            p = doc.add_paragraph(style='BodyText')
            run = p.add_run(code_text)
        elif element.name == 'table':
            rows = element.find_all('tr')
            if not rows: continue

            num_cols = len(rows[0].find_all(['th', 'td']))
            if num_cols == 0: continue
            
            table = doc.add_table(rows=1, cols=num_cols)
            table.style = 'Table Grid'
            
            header_cells = rows[0].find_all(['th', 'td'])
            for i, cell in enumerate(header_cells):
                table.cell(0, i).text = cell.get_text(strip=True)
            
            for row_data in rows[1:]:
                row_cells_data = row_data.find_all('td')
                new_row = table.add_row().cells
                for i, cell in enumerate(row_cells_data):
                    if i < num_cols:
                        new_row[i].text = cell.get_text(strip=True)
    try:
        doc.save(word_path)
    except Exception as e:
        print(f"Error saving Word document to {word_path}: {e}")

def update_lora_and_explanation(selected_model):
    """ÂΩìÊ®°ÂûãÊîπÂèòÊó∂ÔºåÂè™Êõ¥Êñ∞LoraÊ®°ÂûãÂàóË°®ÂíåÊ®°Âûã‰ªãÁªç„ÄÇ"""
    model_path = available_models.get(selected_model)
    lora_list = ['']
    model_explanation = "Ê®°ÂûãË∑ØÂæÑÊú™ÊâæÂà∞ÊàñREADME.mdÁº∫Â§±„ÄÇ"

    if model_path:
        # Êõ¥Êñ∞Ê®°Âûã‰ªãÁªç
        readme_path = os.path.join(model_path, 'README.md')
        if os.path.isfile(readme_path):
            try:
                with open(readme_path, 'r', encoding='utf-8') as file:
                    model_explanation = file.read()
            except Exception as e:
                model_explanation = f"ËØªÂèñREADME.mdÊó∂Âá∫Èîô: {e}"
        
        # Êõ¥Êñ∞LoraÊ®°ÂûãÂàóË°®
        try:
            lora_list.extend([f for f in os.listdir(model_path) if
                                os.path.isdir(os.path.join(model_path, f)) and not f.startswith('.') and not f.startswith('_')])
        except Exception as e:
            print(f"ÂàóÂá∫LoraÊ®°ÂûãÊó∂Âá∫Èîô {model_path}: {e}")
            
    return gr.Dropdown(choices=lora_list, value=''), gr.Textbox(value=model_explanation)

def translate_excel_fixed_languages(input_file, selected_model = 'gpt-4.1-mini', max_workers=10) -> Tuple[str, Optional[str]]:
    """
    ‰ΩøÁî® OpenAI API Âπ∂Ë°åÁøªËØë Excel Êñá‰ª∂‰∏≠ÁöÑÊåáÂÆöÂàó„ÄÇ

    ËØ•ÂáΩÊï∞‰∏•Ê†ºÈÅµÂæ™Âõ∫ÂÆöÁöÑÂàóÂêçÔºàÁÆÄ‰Ωì‰∏≠Êñá(Ê∫ê), English, ÁπÅÈ´î‰∏≠ÊñáÔºâ‰Ωú‰∏∫ÁøªËØëÂèÇËÄÉÔºå
    Âπ∂Â∞ÜÁªìÊûúÂ°´ÂÖÖÂà∞ TARGET_COLUMNS ÂÆö‰πâÁöÑÂêÑËØ≠Ë®ÄÂàó‰∏≠„ÄÇ

    Args:
        input_file: Gradio UI ‰∏ä‰º†ÁöÑÊñá‰ª∂ÂØπË±°„ÄÇ
        max_workers (int): Âπ∂ÂèëÂ§ÑÁêÜÁöÑÁ∫øÁ®ãÊï∞„ÄÇ

    Returns:
        A tuple containing:
        - str: Â§ÑÁêÜËøáÁ®ãÂíåÁªìÊûúÁöÑÁä∂ÊÄÅ‰ø°ÊÅØ„ÄÇ
        - Optional[str]: Â§ÑÁêÜÊàêÂäüÂêéËæìÂá∫Êñá‰ª∂ÁöÑË∑ØÂæÑÔºåÂ§±Ë¥•Âàô‰∏∫ None„ÄÇ
    """
    SIMPLE_COLUMN_NAME = "ÁÆÄ‰Ωì‰∏≠Êñá(Ê∫ê)"
    ENGLISH_COLUMN_NAME = "English"
    TRANS_COLUMN_NAME = "ÁπÅÈ´î‰∏≠Êñá"

    # ÈúÄË¶ÅÁøªËØëÁöÑÁõÆÊ†áËØ≠Ë®ÄÂàóÂêçÂàóË°®
    TARGET_COLUMNS = [
        "Âç∞Â∞ºËØ≠", "ÂåàÁâôÂà©ËØ≠", "Ëë°ËêÑÁâôËØ≠", "Ê≥∞ËØ≠", "ÂúüËÄ≥ÂÖ∂ËØ≠", "Ë∂äÂçóËØ≠", "‰øÑËØ≠",
        "ÈòøÊãâ‰ºØËØ≠", "Ëä¨ÂÖ∞ËØ≠", "‰∏πÈ∫¶ËØ≠", "Ëç∑ÂÖ∞ËØ≠", "Ê≥¢ÂÖ∞ËØ≠", "Ê≥ïËØ≠", "Âæ∑ËØ≠",
        "Êó•ËØ≠", "Êå™Â®ÅËØ≠", "Â∏å‰ºØÊù•ËØ≠", "Èü©ËØ≠", "Ë•øÁè≠ÁâôËØ≠", "Êç∑ÂÖãËØ≠", "ÊÑèÂ§ßÂà©ËØ≠",
        "ÁëûÂÖ∏ËØ≠", "Â∏åËÖäËØ≠", "È©¨Êù•ËØ≠", "ÊñØÊ¥õ‰ºêÂÖãËØ≠", "Êü¨ÂüîÂØ®ËØ≠", "ÁΩóÈ©¨Â∞º‰∫öËØ≠",
        "ÂÖãÁΩóÂú∞‰∫öËØ≠", "‰πåÂÖπÂà´ÂÖãËØ≠", "ÁºÖÁî∏ËØ≠"
    ]
    if not input_file:
        return "ÈîôËØØÔºöËØ∑ÂÖà‰∏ä‰º†‰∏Ä‰∏™ExcelÊñá‰ª∂„ÄÇ", None
    
    client = OpenAI()

    start_time = time.time()
    file_path = input_file.name
    status_messages = [f"‚ñ∂ ÂºÄÂßãÂ§ÑÁêÜÊñá‰ª∂: {os.path.basename(file_path)}"]

    try:
        # 1. ‰ΩøÁî® pandas ËØªÂèñ Excel Êñá‰ª∂
        df = pd.read_excel(file_path)
        status_messages.append(f"‚úî ÊàêÂäüËØªÂèñ Excel Êñá‰ª∂ÔºåÂÖ± {len(df)} Ë°åÊï∞ÊçÆ„ÄÇ")

        # Ê£ÄÊü•ÂøÖÈúÄÁöÑÂàóÊòØÂê¶Â≠òÂú®
        required_columns = [SIMPLE_COLUMN_NAME, ENGLISH_COLUMN_NAME, TRANS_COLUMN_NAME]
        if not all(col in df.columns for col in required_columns):
            missing_cols = [col for col in required_columns if col not in df.columns]
            return f"ÈîôËØØÔºöËæìÂÖ•Êñá‰ª∂Áº∫Â∞ëÂøÖÈúÄÁöÑÂàó: {', '.join(missing_cols)}„ÄÇËØ∑Ê£ÄÊü•Êñá‰ª∂Ê†ºÂºè„ÄÇ", None

        # Á°Æ‰øùÁõÆÊ†áÂàóÂ≠òÂú®ÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®ÂàôÂàõÂª∫
        for col in TARGET_COLUMNS:
            if col not in df.columns:
                df[col] = None
        
        # 2. ‰æùÊ¨°Â§ÑÁêÜÊØè‰∏ÄÁßçÁõÆÊ†áËØ≠Ë®Ä
        all_target_languages = [TRANS_COLUMN_NAME] + TARGET_COLUMNS
        total_languages = len(all_target_languages)

        for lang_idx, target_lang_column in enumerate(all_target_languages):
            status_messages.append(f"\n--- ({lang_idx + 1}/{total_languages}) Ê≠£Âú®Â§ÑÁêÜ: {target_lang_column} ---")
            print(f"\n--- Processing: {target_lang_column} ---")

            # ÂàùÂßãÂåñÁî®‰∫éÁºìÂ≠òÊú¨Ê¨°ËØ≠Ë®ÄÁøªËØëÁªìÊûúÁöÑÂ≠óÂÖ∏
            translation_cache = {}

            def generate_translation(index, row_data):
                """‰∏∫ÂçïË°åÊï∞ÊçÆÁîüÊàêÁøªËØëÁöÑÊ†∏ÂøÉÂáΩÊï∞"""
                # Â¶ÇÊûúÁõÆÊ†áÂçïÂÖÉÊ†ºÂ∑≤ÊúâÂÜÖÂÆπÔºåÂàôÁõ¥Êé•Ë∑≥Ëøá
                if pd.notna(row_data.get(target_lang_column)):
                    return index, None, None # ËøîÂõû None Ë°®Á§∫Êó†ÈúÄÊõ¥Êñ∞

                # ‰ΩøÁî®Ëã±ÊñáÂéüÊñá‰Ωú‰∏∫ÁºìÂ≠òÁöÑ key
                english_text = str(row_data[ENGLISH_COLUMN_NAME])
                
                # Ê£ÄÊü•ÁºìÂ≠ò
                if english_text in translation_cache:
                    return index, translation_cache[english_text], "cache"

                # ÁâπÊÆäÂ§ÑÁêÜÔºöÁπÅ‰Ωì‰∏≠ÊñáÁõ¥Êé•ËΩ¨Êç¢Ôºå‰∏çË∞ÉÁî® API
                if target_lang_column == TRANS_COLUMN_NAME:
                    simplified_text = str(row_data[SIMPLE_COLUMN_NAME])
                    translated_text = zhconv.convert(simplified_text, 'zh-tw')
                    translation_cache[english_text] = translated_text
                    return index, translated_text, "zhconv"

                # Ë∞ÉÁî® OpenAI API ËøõË°åÁøªËØë
                try:
                    # Few-shot promptÔºå‰∏∫Ê®°ÂûãÊèê‰æõ‰∏ä‰∏ãÊñáÁ§∫‰æãÔºåÊèêÂçáÁøªËØëË¥®Èáè
                    completion = client.chat.completions.create(
                        model=selected_model,
                        messages=[
                            {"role": "user", "content": f"Translate the following sentence or word from English to {SIMPLE_COLUMN_NAME}: {english_text}, please directly translate it and do not output any extra content"},
                            {"role": "assistant", "content": str(row_data[SIMPLE_COLUMN_NAME])},
                            {"role": "user", "content": f"Translate the following sentence or word from English to {TRANS_COLUMN_NAME}: {english_text}, please directly translate it and do not output any extra content"},
                            {"role": "assistant", "content": zhconv.convert(str(row_data[SIMPLE_COLUMN_NAME]), 'zh-tw')},
                            {"role": "user", "content": f"Translate the following sentence or word from English to {target_lang_column}: {english_text}, please directly translate it and do not output any extra content"}
                        ],
                        temperature=0.0,
                        max_tokens=200
                    )
                    translated_text = completion.choices[0].message.content.strip()
                    # Â≠òÂÖ•ÁºìÂ≠ò
                    translation_cache[english_text] = translated_text
                    return index, translated_text, "api"
                except Exception as api_error:
                    return index, f"API_ERROR: {api_error}", "error"

            # 3. ‰ΩøÁî®Á∫øÁ®ãÊ±†Âπ∂ÂèëÂ§ÑÁêÜ
            tasks_to_process = [(index, row) for index, row in df.iterrows() if pd.isna(row.get(target_lang_column))]
            if not tasks_to_process:
                status_messages.append(f"‚úî '{target_lang_column}' ÂàóÂ∑≤ÂÖ®ÈÉ®ÁøªËØëÔºåË∑≥Ëøá„ÄÇ")
                print(f"'{target_lang_column}' column is already fully translated. Skipping.")
                continue

            status_messages.append(f"ÊâæÂà∞ {len(tasks_to_process)} ‰∏™ÂæÖÁøªËØëÊù°ÁõÆÔºåÂºÄÂßãÂ§ÑÁêÜ...")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Êèê‰∫§‰ªªÂä°
                future_to_index = {executor.submit(generate_translation, index, row): index for index, row in tasks_to_process}

                # Áõ¥Êé•Ëø≠‰ª£Â∑≤ÂÆåÊàêÁöÑ‰ªªÂä°Ôºå‰∏çÂÜçÊòæÁ§∫ËøõÂ∫¶Êù°
                for future in as_completed(future_to_index):
                    index, result, source = future.result()
                    if result is not None and source != "error":
                        df.at[index, target_lang_column] = result
                    elif source == "error":
                        print(f"Error processing row {index}: {result}")


        # 4. ‰øùÂ≠òÂà∞Êñ∞Êñá‰ª∂
        processed_dir = os.path.join(os.path.dirname(file_path), 'processed_openai')
        os.makedirs(processed_dir, exist_ok=True)
        base_name = os.path.basename(file_path)
        name, ext = os.path.splitext(base_name)
        # Âú®Êñá‰ª∂Âêç‰∏≠Âä†ÂÖ•Êó∂Èó¥Êà≥Èò≤Ê≠¢Ë¶ÜÁõñ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file_path = os.path.join(processed_dir, f"{name}_translated_{timestamp}{ext}")
        
        df.to_excel(output_file_path, index=False)

    except Exception as e:
        error_message = f"Â§ÑÁêÜExcelÊñá‰ª∂Êó∂ÂèëÁîü‰∏•ÈáçÈîôËØØ: {e}"
        status_messages.append(f"\n‚ùå {error_message}")
        print(error_message)
        return "\n".join(status_messages), None

    end_time = time.time()
    total_time = int(end_time - start_time)
    status_messages.append(f"\nüéâ ÊâÄÊúâÁøªËØë‰ªªÂä°ÂÆåÊàêÔºÅÊÄªËÄóÊó∂: {total_time}Áßí„ÄÇ")
    status_messages.append(f"‚úî ÁªìÊûúÂ∑≤‰øùÂ≠òËá≥: {os.path.basename(output_file_path)}")
    
    return "\n".join(status_messages), output_file_path

def extract_complex_blocks(md_content: str):
    """
    ‰ΩøÁî®Âç†‰ΩçÁ¨¶ÊèêÂèñMarkdown‰∏≠ÁöÑÂ§çÊùÇÂùóÔºàÂõæÁâá„ÄÅË°®Ê†º„ÄÅ‰ª£Á†ÅÂùóÔºâ„ÄÇ(Ê≠§ÂáΩÊï∞‰∏çÂèò)
    """
    pattern = re.compile(
        r"(!\[.*?\]\(.*?\))|"
        r"((?:\|.*\|[\r\n]+)+(?:\|-+\|.*[\r\n]+)+(?:\|.*\|[\r\n]?)*)|"
        r"(```[\s\S]*?```)"
        , re.MULTILINE)
    blocks = {}
    def replacer(match):
        placeholder = f"__COMPLEX_BLOCK_{len(blocks)}__"
        blocks[placeholder] = match.group(0)
        return placeholder
    clean_md = pattern.sub(replacer, md_content)
    return clean_md, blocks

def restore_complex_blocks(translated_content: str, blocks: dict) -> str:
    """
    Â∞ÜÂç†‰ΩçÁ¨¶ÊõøÊç¢ÂõûÂÖ∂ÂéüÂßãÁöÑÂ§çÊùÇÂùóÂÜÖÂÆπ„ÄÇ(Ê≠§ÂáΩÊï∞‰∏çÂèò)
    """
    for placeholder, original_block in blocks.items():
        translated_content = translated_content.replace(placeholder, original_block)
    return translated_content


def translate_markdown_folder(translating_files: list[NamedString],
                            selected_model: Optional[str], selected_lora_model: Optional[str],
                            selected_gpu: Optional[str], batch_size: int,
                            original_language: Optional[str], target_language: Optional[str]):
    start_time = time.time()
    if not translating_files:
        return "No files uploaded", None

    folder_path = os.path.dirname(translating_files[0].name)
    processed_files = []
    temp_image_dir = os.path.join(folder_path, "temp_images_from_docx")

    processed_folder = os.path.join(folder_path, 'processed')
    os.makedirs(processed_folder, exist_ok=True)
    if os.path.exists(temp_image_dir):
        shutil.rmtree(temp_image_dir)
    os.makedirs(temp_image_dir, exist_ok=True)

    for input_file in translating_files:
        file_path = input_file.name
        file_name, file_ext = os.path.splitext(os.path.basename(file_path))
        output_file_path = None

        try:
            if file_ext.lower() == '.pptx':
                def extract_text_from_shape(shape, run_list, text_list):
                    """ÈÄíÂΩíÊèêÂèñÊâÄÊúâÊñáÊú¨ÔºåÂåÖÊã¨ÊñáÊú¨Ê°Ü„ÄÅË°®Ê†ºÂíåÂµåÂ•óÂΩ¢Áä∂"""
                    if hasattr(shape, "text_frame") and shape.text_frame is not None:
                        # Â§ÑÁêÜÊôÆÈÄöÊñáÊú¨Ê°Ü
                        for paragraph in shape.text_frame.paragraphs:
                            for run in paragraph.runs:
                                run_list.append(run)
                                text_list.append(run.text)
                    elif getattr(shape, "has_table", False):
                        # ‰ªÖÂΩì shape Á°ÆÂÆûÂåÖÂê´Ë°®Ê†ºÊó∂ËøõË°åÂ§ÑÁêÜ
                        table = shape.table
                        for row in table.rows:
                            for cell in row.cells:
                                if cell.text_frame is not None:
                                    for paragraph in cell.text_frame.paragraphs:
                                        for run in paragraph.runs:
                                            run_list.append(run)
                                            text_list.append(run.text)
                    elif hasattr(shape, "shapes"):
                        # Â§ÑÁêÜÂµåÂ•óÁöÑ grouped shapes
                        for sub_shape in shape.shapes:
                            extract_text_from_shape(sub_shape, run_list, text_list)

                prs = Presentation(file_path)
                run_list = []
                text_list = []

                for slide in prs.slides:
                    for shape in slide.shapes:
                        extract_text_from_shape(shape, run_list, text_list)  # Á°Æ‰øùÊèêÂèñÊâÄÊúâÊñáÊú¨

                # ÁøªËØëÊñáÊú¨
                translated_segments = translate(text_list, selected_model, selected_lora_model, selected_gpu,
                                                batch_size, original_language, target_language)

                # ÊõøÊç¢ÂéüÂßãÊñáÊú¨
                for run, translated in zip(run_list, translated_segments):
                    run.text = " " + translated[0]["generated_translation"]

                # ‰øùÂ≠ò PPTX
                output_file_path = os.path.join(processed_folder, os.path.basename(file_name + '.pptx'))
                prs.save(output_file_path)
                processed_files.append(output_file_path)

            elif file_ext.lower() in ['.xlsx', '.xls']:
                print(f"Processing Excel file: {file_path}")
                
                workbook = openpyxl.load_workbook(file_path)
                texts_to_translate = []
                cell_locations = []

                # ‰øÆÊîπÔºö‰∏çÂÜçÊ£ÄÊü•Á±ªÂûãÔºåÂè™Ë¶ÅÊúâÂÄºÂ∞±ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤ËøõË°åÁøªËØë
                for sheet_name in workbook.sheetnames:
                    sheet = workbook[sheet_name]
                    for row_idx, row in enumerate(sheet.iter_rows()):
                        for col_idx, cell in enumerate(row):
                            if cell.value:  # Âè™Ë¶ÅÂçïÂÖÉÊ†º‰∏ç‰∏∫Á©∫
                                texts_to_translate.append(str(cell.value)) # Âº∫Âà∂ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤
                                cell_locations.append((sheet_name, cell.row, cell.column))
                
                if texts_to_translate:
                    target_lang_list = [target_language] if isinstance(target_language, str) else target_language
                    
                    # Ë∞ÉÁî®ÁøªËØëÔºå‰∏çÂÜçÂØπÁªìÊûúËøõË°å‰ªª‰ΩïÊ£ÄÊü•
                    translated_results = translate(texts_to_translate, selected_model, selected_lora_model, selected_gpu,
                                                batch_size, original_language, target_lang_list)

                    # ‰øÆÊîπÔºö‰∏çÂÜçÈ™åËØÅÁªìÊûúÔºåÁõ¥Êé•ÂÜôÂÖ•„ÄÇÂ¶ÇÊûúÊ†ºÂºèÈîôËØØÔºåÊ≠§Â§ÑÂ∞ÜÂºïÂèëÂºÇÂ∏∏„ÄÇ
                    for i, location in enumerate(cell_locations):
                        translated_text = translated_results[i][0]['generated_translation']
                        sheet_name, row, col = location
                        workbook[sheet_name].cell(row=row, column=col, value=translated_text)
                
                output_filename_base = os.path.basename(file_name + '_translated')
                output_file_path = os.path.join(processed_folder, output_filename_base + file_ext)
                workbook.save(output_file_path)

            elif file_ext.lower() in ['.docx', '.md']:
                md_content = ""
                file_is_word = False
                if file_ext.lower() == '.docx':
                    file_is_word = True
                    md_content = word_to_markdown(file_path, output_dir=temp_image_dir)
                elif file_ext.lower() == '.md':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        md_content = f.read()

                clean_md, protected_blocks = extract_complex_blocks(md_content)
                
                # ‰øÆÊîπÔºö‰∏çÂÜçËøáÊª§Á©∫ÊÆµËêΩÔºå‰ΩÜÈÄöÂ∏∏Êåâ\n\nÂàÜÂâ≤Âêé‰∏ç‰ºöÊúâÂÆåÂÖ®Á©∫ÁöÑÂÖÉÁ¥†
                text_to_translate = [p for p in clean_md.split('\n\n')]
                translated_content = clean_md

                if text_to_translate:
                    target_lang_list = [target_language] if isinstance(target_language, str) else target_language
                    
                    # Ë∞ÉÁî®ÁøªËØëÔºå‰∏çÂÜçÂØπÁªìÊûúËøõË°å‰ªª‰ΩïÊ£ÄÊü•
                    translated_results = translate(text_to_translate, selected_model, selected_lora_model, selected_gpu,
                                                batch_size, original_language, target_lang_list)

                    # ‰øÆÊîπÔºö‰∏çÂÜçÈ™åËØÅÁªìÊûúÔºåÁõ¥Êé•ÂàõÂª∫Êò†Â∞Ñ„ÄÇÂ¶ÇÊûúÊ†ºÂºèÈîôËØØÔºåÊ≠§Â§ÑÂ∞ÜÂºïÂèëÂºÇÂ∏∏„ÄÇ
                    translation_map = {
                        original: result_list[0]['generated_translation']
                        for original, result_list in zip(text_to_translate, translated_results)
                    }

                    temp_translated_content = []
                    for para in clean_md.split('\n\n'):
                        # ‰ΩøÁî®getÊñπÊ≥ïÊèê‰æõ‰∏Ä‰∏™ÈªòËÆ§ÂõûÈÄÄÔºåËøôÊòØÊúÄÂêéÁöÑ„ÄÅÊúÄÂ∞èÁöÑ‰øùÊä§
                        temp_translated_content.append(translation_map.get(para, para))
                    translated_content = '\n\n'.join(temp_translated_content)
                
                final_md_content = restore_complex_blocks(translated_content, protected_blocks)
                
                output_filename_base = os.path.basename(file_name + '_translated')
                if file_is_word:
                    output_file_path = os.path.join(processed_folder, output_filename_base + '.docx')
                    markdown_to_word(final_md_content, output_file_path, image_base_dir=temp_image_dir)
                else:
                    output_file_path = os.path.join(processed_folder, output_filename_base + '.md')
                    with open(output_file_path, 'w', encoding='utf-8') as f:
                        f.write(final_md_content)
            else:
                print(f"Skipping unsupported file type: {file_path}")
                continue

            if output_file_path and os.path.exists(output_file_path):
                processed_files.append(output_file_path)

        except Exception as e:
            # ÂºÇÂ∏∏Â§ÑÁêÜ‰ªçÁÑ∂‰øùÁïôÔºåËøôÊòØÊúÄÂü∫Á°ÄÁöÑ‰øùÊä§ÔºåÈò≤Ê≠¢‰∏Ä‰∏™Êñá‰ª∂ÁöÑÂ§±Ë¥•ÂØºËá¥Êï¥‰∏™Â∫îÁî®Â¥©Ê∫É
            print(f"CRITICAL ERROR processing file {file_path}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # --- Zipping Âíå Cleanup ÈÄªËæë‰øùÊåÅ‰∏çÂèò ---
    if not processed_files:
        if os.path.exists(temp_image_dir):
            shutil.rmtree(temp_image_dir)
        return "No files processed successfully.", None
    
    output_path = None
    
    # Â¶ÇÊûúÂè™Êúâ‰∏Ä‰∏™Êñá‰ª∂Ë¢´ÊàêÂäüÂ§ÑÁêÜÔºåÁõ¥Êé•ËøîÂõûËØ•Êñá‰ª∂Ë∑ØÂæÑ
    if len(processed_files) == 1:
        output_path = processed_files[0]
    
    # Â¶ÇÊûúÊúâÂ§ö‰∏™Êñá‰ª∂ÔºåÂàôÂ∞ÜÂÆÉ‰ª¨ÂéãÁº©
    else:
        zip_filename = os.path.join(folder_path, "processed_files.zip")
        try:
            with zipfile.ZipFile(zip_filename, 'w') as zipf:
                for file in processed_files:
                    if os.path.exists(file):
                        zipf.write(file, os.path.basename(file))
            output_path = zip_filename
        except Exception as e:
            # Â¶ÇÊûúÂéãÁº©Â§±Ë¥•Ôºå‰πüÈúÄË¶ÅÊ∏ÖÁêÜÂπ∂ËøîÂõûÈîôËØØ
            if os.path.exists(temp_image_dir):
                shutil.rmtree(temp_image_dir)
            return f"Error creating zip file: {e}", None

    # Âú®ÂáΩÊï∞ÊàêÂäüËøîÂõûÂâçÔºåÊ∏ÖÁêÜ‰∏¥Êó∂ÂõæÁâáÁõÆÂΩï
    if os.path.exists(temp_image_dir):
        shutil.rmtree(temp_image_dir)

    end_time = time.time()
    duration = int(end_time - start_time)
    
    # ËøîÂõûÊúÄÁªàÁöÑÊàêÂäü‰ø°ÊÅØÂíåÊñá‰ª∂Ë∑ØÂæÑÔºàÂçï‰∏™Êñá‰ª∂ÊàñÂéãÁº©ÂåÖÔºâ
    return f"Total process time: {duration}s. {len(processed_files)} file(s) processed.", output_path

def glossary_check(input_folder, start_row, end_row, original_column, reference_column, translated_column,
                    row_selection, remark_column) -> tuple[str, Optional[str]]: # Return tuple[status, filepath]
    # ... (glossary_check implementation remains the same)
    def contains_special_string(sentence):
        # ÂÆö‰πâÁâπÊÆäÂ≠óÁ¨¶‰∏≤ÁöÑÊ≠£ÂàôË°®ËææÂºèÊ®°ÂºèÂ≠óÂÖ∏
        patterns = {
            "Content within <% ... %> should not be translated": r"<%.*?%>",  # Match <% ... %>
            "Special symbol %s should be contained": r"%s",  # Match %s
            "Special symbols {0}, {1}, {2}, etc., should not be translated": r"{\d+}",  # Match {0}, {1}, {2}, etc.
            "Special symbol %d should be contained": r"%d",  # Match %d
            "String {counts} should not be translated": r"{counts}",  # Match {counts}
            "String (value) should not be translated": r"\(value\)",  # Match (value)
            "String (Value) should not be translated": r"\(Value\)",  # Match (Value)
            "String (text) should not be translated": r"\(text\)",  # Match (text)
            "String (Text) should not be translated": r"\(Text\)",  # Match (Text)
            "String (message) should not be translated": r"\(message\)",  # Match (message)
            "String (Message) should not be translated": r"\(Message\)",  # Match (Message)
            "String (group) should not be translated": r"\(group\)",  # Match (group)
            "String (Group) should not be translated": r"\(Group\)",  # Match (Group)
            "Content within &{...}& should not be translated": r"&{.*?}&",  # Match &{...}&
            "String {} should be contained": r"{}",  # Match {}
            "Content within #...# should not be translated": r"#.*?#",  # Match #...#
            "Content within {{...}} should not be translated": r"{{.*?}}",  # Match {{...}}
            "Consecutive uppercase letters (AR, AP, SKU) should be contained": r"\b[A-Z]{2,}\b", # Use \b for word boundaries
            "CamelCase words (e.g., ServiceCode, LocStudio) should be contained": r"\b(?:[A-Z][a-z]+){2,}\b", # Use \b
            "Full links http:// should be contained": r"http://",  # Match full links containing "http://"
            "Full links https:// should be contained": r"https://",  # Match full links containing "https://"
            "Full file paths E:\\, D:\\, C:\\ should be contained": r"\b[CDE]:\\", # Use \b
            "Formula-like strings such as datediff(.*?,.*?,.*?) should not be translated": r"datediff\(.*?,.*?,.*?\)",
            "Strings like @BusinessFunction. ... @ should not be translated": r"@‰∏öÂä°ÂáΩÊï∞\..*?@",
            "CamelCase words starting with a lowercase letter (e.g., serviceCode, locStudio) should not be translated": r"\b[a-z]+[A-Z][a-zA-Z]*\b", # Use \b
            "String ${label} should not be translated": r"\$\{label\}",
            "String [${enum}] should not be translated": r"\[\$\{enum\}\]",
            "String ${max} should not be translated": r"\$\{max\}",
            "String ${min} should not be translated": r"\$\{min\}",
            "String ${len} should not be translated": r"\$\{len\}",
            "String ${pattern} should not be translated": r"\$\{pattern\}",
            "String [{{fievent}}] should not be translated": r"\[\{\{fievent\}\}\]",
            "String [{{accBook}}] should not be translated": r"\[\{\{accBook\}\}\]",
        }

        reasons = []  # Áî®‰∫éÂ≠òÂÇ®ÂåπÈÖçÁöÑÊù°ÁõÆ
        matched_strings = []  # Áî®‰∫éÂ≠òÂÇ®Ë¢´ËØÜÂà´ÁöÑÂ≠óÁ¨¶‰∏≤
        # Ensure sentence is a string
        sentence_str = str(sentence) if sentence is not None else ""

        for reason, pattern in patterns.items():
            try:
                matches = re.findall(pattern, sentence_str)
                if matches:
                    # Avoid adding duplicate reasons if multiple matches for same pattern
                    if reason not in reasons:
                            reasons.append(reason)
                    # Add unique matches for this pattern
                    for match in matches:
                            if match not in matched_strings:
                                matched_strings.append(match)
            except Exception as e:
                print(f"Regex error for pattern '{pattern}' on sentence '{sentence_str[:50]}...': {e}")


        return {
            "contains_special_string": bool(reasons),  # Â¶ÇÊûú reasons ÂàóË°®‰∏ç‰∏∫Á©∫ÔºåË°®Á§∫ÂåπÈÖç
            "reason": reasons,  # ËøîÂõûÊâÄÊúâÂåπÈÖçÊù°ÁõÆ (unique reasons)
            "matched_strings": matched_strings  # ËøîÂõûÊâÄÊúâË¢´ËØÜÂà´ÁöÑÂ≠óÁ¨¶‰∏≤ (unique matches)
        }

    result = []
    excel_writer = ExcelFileWriter()
    processed_files = []
    output_zip_path = None # Initialize zip path

    if not input_folder:
            return "Error: No folder/files provided.", None

    try:
            folder_path = os.path.dirname(input_folder[0].name) # Get dir from first file
            processed_folder = os.path.join(folder_path, 'processed_glossary_check')
            os.makedirs(processed_folder, exist_ok=True)
    except Exception as e:
            return f"Error creating processed folder: {e}", None


    for input_file in input_folder:
        file_path = input_file.name
        file_name_base = os.path.basename(file_path)
        file_name, file_ext = os.path.splitext(file_name_base)

        if file_ext.lower() == '.xlsx':
            current_end_row = end_row
            try:
                if row_selection == "ÊâÄÊúâË°å":
                    current_end_row = FileReaderFactory.count_rows(file_path)

                reader, fp = FileReaderFactory.create_reader(file_path) # fp might be None or file pointer
                original_inputs = reader.extract_text(file_path, original_column, start_row, current_end_row)
                reference_inputs = reader.extract_text(file_path, reference_column, start_row, current_end_row)
                translated_inputs = reader.extract_text(file_path, translated_column, start_row, current_end_row)
                if fp: # Close file pointer if factory returned one
                        fp.close()

            except Exception as e:
                result.append(f"Error reading {file_name_base}: {e}")
                continue # Skip to next file

            result.append(f"Checking {file_name_base}:")
            outputs = [] # Remarks to write back to Excel
            max_len = max(len(original_inputs), len(reference_inputs), len(translated_inputs))
            min_len = min(len(original_inputs), len(reference_inputs), len(translated_inputs))
            if max_len != min_len:
                    result.append(f"\tWarning: Column lengths differ ({len(original_inputs)}, {len(reference_inputs)}, {len(translated_inputs)}). Processing up to shortest length: {min_len}")

            for index in range(min_len):
                original_input = original_inputs[index]
                reference_input = reference_inputs[index]
                translated_input = translated_inputs[index]
                remark = "" # Start with empty remark

                # Ensure inputs are strings for comparison and analysis
                original_str = str(original_input) if original_input is not None else ""
                reference_str = str(reference_input) if reference_input is not None else ""
                translated_str = str(translated_input) if translated_input is not None else ""


                special_check_result = contains_special_string(original_str)

                if special_check_result["contains_special_string"]:
                    missed_matches_info = [] # Store tuples of (missed_string, reason)
                    found_in_translation = True # Assume found initially

                    for matched_string in special_check_result["matched_strings"]:
                        # Check if the specific matched string is missing in translation
                        if matched_string not in translated_str:
                            # If missing in translation, check if it was also missing in reference (optional check)
                            if matched_string not in reference_str:
                                # Optional: Log or handle cases missing in both ref and translation differently
                                # print(f"Row {start_row + index}: '{matched_string}' missing in translation and reference.")
                                pass # Currently, we only penalize if missing in translation but present in original
                            else:
                                # It's missing in translation but was in original (and reference implies it should be kept)
                                found_in_translation = False
                                # Find the reason associated with this specific string (might be multiple reasons)
                                associated_reasons = [r for r, p in contains_special_string(original_str)["reason"].items() if re.search(p, matched_string)]
                                reason_text = associated_reasons[0] if associated_reasons else "Unknown reason" # Get first reason
                                missed_matches_info.append((matched_string, reason_text))


                    if not found_in_translation:
                        # Format the remark string
                        missed_items_str = ', '.join([f"'{info[0]}'" for info in missed_matches_info])
                        reasons_str = ', '.join(set([info[1] for info in missed_matches_info])) # Unique reasons
                        remark += f"MISSED: {missed_items_str}; REASON: {reasons_str}"
                        result.append(
                            f"\tROW: {start_row + index}, MISSED: {missed_items_str}, REASON: {reasons_str}")
                    else:
                            # All special strings found, add a generic remark if needed
                            remark += "SPECIAL_STRINGS_OK"

                # Token length check (optional, added to remark)
                try:
                    # Ensure tokenizer is loaded
                    if 'tokenizer' in globals() and tokenizer:
                            token_count = len(tokenizer.encode(original_str))
                            if token_count > 40: # Your threshold
                                length_remark = "EXCEEDS_40_TOKENS"
                                if remark: # Append to existing remark
                                    remark += f"; {length_remark}"
                                else:
                                    remark = length_remark
                                result.append(f"\tROW: {start_row + index}, {length_remark} ({token_count} tokens)")
                    else:
                        print("Warning: Tokenizer not loaded, skipping length check.")
                except Exception as e:
                    print(f"Error during tokenization for row {start_row + index}: {e}")


                outputs.append(remark if remark else "") # Append remark or empty string


            # Ensure outputs list has the correct length (up to min_len)
            if len(outputs) != min_len:
                print(f"Warning: Length mismatch in remarks generation for {file_name_base}. Expected {min_len}, got {len(outputs)}.")
                # Pad or truncate if necessary, though ideally loop range handles this.
                outputs = outputs[:min_len] + [""] * (min_len - len(outputs))


            # Write remarks back to the Excel file
            try:
                # write_list expects the file path, list of remarks, target column, start row, end row (exclusive?)
                # Adjust end row for write_list if it's exclusive: current_end_row + 1 ? Check definition.
                # Assuming end_row in write_list is inclusive index matching read logic.
                output_file = excel_writer.write_list(file_path, outputs, remark_column, start_row, current_end_row)

                # Move the modified file to the processed folder
                processed_file_path = os.path.join(processed_folder, os.path.basename(output_file))
                # shutil.move might fail if source and destination are the same drive/different mounts sometimes
                # Safer to copy then delete source, or ensure move works. Let's assume move is ok.
                shutil.move(output_file, processed_file_path)
                processed_files.append(processed_file_path)
                result.append(f"{file_name_base} check completed. Results saved to processed folder.")

            except Exception as e:
                result.append(f"Error writing remarks or moving file for {file_name_base}: {e}")
                # Clean up partially written file if possible (output_file might exist)
                if 'output_file' in locals() and os.path.exists(output_file):
                    try: os.remove(output_file)
                    except OSError: pass


        else:
            result.append(f"Skipping non-Excel file: {file_name_base}")


    # --- Create Zip of Processed Files ---
    if processed_files:
        zip_filename_base = "glossary_check_results.zip"
        output_zip_path = os.path.join(folder_path, zip_filename_base) # Save zip in original upload dir
        try:
            with zipfile.ZipFile(output_zip_path, 'w') as zipf:
                for file in processed_files:
                    if os.path.exists(file):
                        zipf.write(file, os.path.basename(file))
                        print(f"Adding {os.path.basename(file)} to zip.")
                    else:
                        print(f"Warning: File {file} not found for zipping.")
            result.append(f"Processed files zipped to {zip_filename_base}")
        except Exception as e:
            result.append(f"Error creating zip file: {e}")
            output_zip_path = None # Indicate zip creation failed
    else:
        result.append("No files were processed successfully.")


    return "\n".join(result), output_zip_path # Return status string and path to zip (or None)


def webui():

    def update_choices(selected_model):
        model_path = available_models.get(selected_model) # ‰ΩøÁî® .get() Êõ¥ÂÆâÂÖ®
        original_language_choices = []
        target_language_choices = []
        lora_list = ['']
        model_explanation = "Model path not found or README.md missing."

        if model_path:
            support_language_path = os.path.join(model_path, 'support_language.json')
            readme_path = os.path.join(model_path, 'README.md')

            if os.path.isfile(readme_path):
                try:
                    with open(readme_path, 'r', encoding='utf-8') as file:
                        model_explanation = file.read()
                except Exception as e:
                    print(f"Error reading README.md: {e}")
                    model_explanation = f"Error reading README.md: {e}"

            try:
                with open(support_language_path, 'r') as file:
                    support_languages = json.load(file)
                    original_language_choices = support_languages.get("original_language", [])
                    target_language_choices = support_languages.get("target_language", [])
            except Exception as e:
                print(f"Error reading support_language.json: {e}")
                # Keep choices empty if file read fails

            try:
                lora_list = [''] + [f for f in os.listdir(model_path) if
                                    os.path.isdir(os.path.join(model_path, f)) and not f.startswith('.') and not f.startswith(
                                        '_')]
            except Exception as e:
                print(f"Error listing lora models in {model_path}: {e}")
                lora_list = [''] # Reset to default if error

        # --- ÈáçË¶Å: ËøîÂõûÊõ¥Êñ∞ÂêéÁöÑÁªÑ‰ª∂ÔºåÂêåÊó∂Á°Æ‰øùÈªòËÆ§ÂÄºÂú® choices ‰∏≠ ---
        # Â¶ÇÊûúÈªòËÆ§ÂÄº‰∏çÂú®Âä®ÊÄÅÂä†ËΩΩÁöÑchoicesÈáåÔºåGradio ÂèØËÉΩÊó†Ê≥ïÊ≠£Á°ÆÊòæÁ§∫ÈªòËÆ§ÂÄº
        # ËøôÈáåÂÅáËÆæ 'Chinese' Âíå 'English' ÈÄöÂ∏∏‰ºöÂ≠òÂú®‰∫éÂä†ËΩΩÁöÑËØ≠Ë®ÄÂàóË°®‰∏≠
        # Â¶ÇÊûúÊ®°Âûã‰∏çÊîØÊåÅËøô‰∫õËØ≠Ë®ÄÔºåÈªòËÆ§ÂÄºÂèØËÉΩ‰∏ç‰ºöË¢´ÈÄâ‰∏≠
        return (gr.Dropdown(choices=original_language_choices, value=default_original_language if default_original_language in original_language_choices else None),
                gr.Dropdown(choices=target_language_choices, value=default_target_language_multi if default_target_language_single in target_language_choices else None, multiselect=True), # Â§ÑÁêÜÂ§öÈÄâÁöÑÊÉÖÂÜµ
                gr.Dropdown(choices=lora_list), # Lora Ê®°ÂûãÈÄöÂ∏∏Ê≤°ÊúâÈªòËÆ§ÂÄºÔºåÈô§‰∫ÜÁ©∫Â≠óÁ¨¶‰∏≤
                model_explanation)

    
    # --- Gradio UI Layout ---
    with gr.Blocks(title="yonyou translator") as interface:
        # Define initial choices based on the default model, if possible
        initial_original_choices = []
        initial_target_choices = []
        initial_lora_choices = ['']
        initial_explanation = "Select a model to see details."

        # Try to pre-populate choices for the default model
        if default_model_name in available_models:
            try:
                 # Call update_choices logic manually once for defaults
                 # This avoids needing a complex Gradio `load` event setup sometimes
                 _model_path = available_models[default_model_name]
                 _sl_path = os.path.join(_model_path, 'support_language.json')
                 _readme_path = os.path.join(_model_path, 'README.md')

                 if os.path.exists(_readme_path):
                      with open(_readme_path, 'r', encoding='utf-8') as f: initial_explanation = f.read()
                 if os.path.exists(_sl_path):
                      with open(_sl_path, 'r') as f:
                           _langs = json.load(f)
                           initial_original_choices = _langs.get("original_language", [])
                           initial_target_choices = _langs.get("target_language", [])
                 try:
                    initial_lora_choices = [''] + [f for f in os.listdir(_model_path) if
                                     os.path.isdir(os.path.join(_model_path, f)) and not f.startswith('.') and not f.startswith('_')]
                 except Exception: pass # Ignore lora errors on initial load

            except Exception as e:
                 print(f"Error pre-loading defaults for {default_model_name}: {e}")
                 # Keep initial_* lists empty/default if error

        # gr.Button("Logout", link="/logout")
        with gr.Tabs():
            with gr.TabItem("Excel Translator"):
                with gr.Row():
                    with gr.Column():
                        input_file = gr.File()
                        with gr.Row():
                            start_row = gr.Number(value=2, label="Ëµ∑ÂßãË°å")
                            end_row = gr.Number(value=100001, label="ÁªàÊ≠¢Ë°å")
                            target_column = gr.Textbox(value="G", label="ÁõÆÊ†áÂàó")
                            start_column = gr.Textbox(value="H", label="ÁªìÊûúÂÜôÂÖ•Âàó")
                        with gr.Row():
                            # Set default model value here
                            selected_model_excel = gr.Dropdown(choices=list(available_models.keys()), label="ÈÄâÊã©Âü∫Ê®°Âûã", value=default_model_name)
                            # Set initial choices and default lora value
                            selected_lora_model_excel = gr.Dropdown(choices=initial_lora_choices, label="ÈÄâÊã©LoraÊ®°Âûã", value='')
                            selected_gpu_excel = gr.Dropdown(choices=available_gpus, label="ÈÄâÊã©GPU", value=available_gpus[0] if available_gpus else None)
                            batch_size_excel = gr.Number(value=10, label="ÊâπÂ§ÑÁêÜÂ§ßÂ∞è")
                        with gr.Row():
                            # Set initial choices and default language values here
                            original_language_excel = gr.Dropdown(choices=initial_original_choices, label="ÂéüÂßãËØ≠Ë®Ä", value=default_original_language)
                            target_languages_excel = gr.Dropdown(choices=initial_target_choices, label="ÁõÆÊ†áËØ≠Ë®Ä", multiselect=True, value=default_target_language_multi)
                        translate_button_excel = gr.Button("Translate")
                    with gr.Column():
                        # Set initial explanation
                        model_explanation_textbox_excel = gr.Textbox(label="Ê®°Âûã‰ªãÁªç", lines=5, value=initial_explanation)
                        output_text_excel = gr.Textbox(label="ËæìÂá∫ÊñáÊú¨")
                        output_file_excel = gr.File(label="ÁøªËØëÊñá‰ª∂‰∏ãËΩΩ")

                # Link change event
                selected_model_excel.change(update_choices,
                                            inputs=[selected_model_excel],
                                            outputs=[original_language_excel, target_languages_excel, selected_lora_model_excel, model_explanation_textbox_excel])
                # Link click event
                translate_button_excel.click(translate_excel,
                                             inputs=[input_file, start_row, end_row, start_column, target_column,
                                                     selected_model_excel, selected_lora_model_excel, selected_gpu_excel, batch_size_excel,
                                                     original_language_excel, target_languages_excel],
                                             outputs=[output_text_excel, output_file_excel])

            with gr.TabItem("Text Translator"):
                 with gr.Row():
                     with gr.Column():
                         input_text_text = gr.Textbox(label="ËæìÂÖ•ÊñáÊú¨", lines=3)
                         with gr.Row():
                              selected_model_text = gr.Dropdown(choices=list(available_models.keys()), label="ÈÄâÊã©Âü∫Ê®°Âûã", value=default_model_name)
                              selected_lora_model_text = gr.Dropdown(choices=initial_lora_choices, label="ÈÄâÊã©LoraÊ®°Âûã", value='')
                              selected_gpu_text = gr.Dropdown(choices=available_gpus, label="ÈÄâÊã©GPU", value=available_gpus[0] if available_gpus else None)
                              batch_size_text = gr.Number(value=1, label="ÊâπÂ§ÑÁêÜÂ§ßÂ∞è", visible=False) # Usually 1 for single text
                         with gr.Row():
                              original_language_text = gr.Dropdown(choices=initial_original_choices, label="ÂéüÂßãËØ≠Ë®Ä", value=default_original_language)
                              target_languages_text = gr.Dropdown(choices=initial_target_choices, label="ÁõÆÊ†áËØ≠Ë®Ä", multiselect=True, value=default_target_language_multi)
                         translate_button_text = gr.Button("Translate")
                     with gr.Column():
                          model_explanation_textbox_text = gr.Textbox(label="Ê®°Âûã‰ªãÁªç", lines=5, value=initial_explanation)
                          output_text_text = gr.Textbox(label="ËæìÂá∫ÊñáÊú¨", lines=5)

                 selected_model_text.change(update_choices,
                                           inputs=[selected_model_text],
                                           outputs=[original_language_text, target_languages_text, selected_lora_model_text, model_explanation_textbox_text])
                 translate_button_text.click(translate,
                                            inputs=[input_text_text, selected_model_text, selected_lora_model_text, selected_gpu_text,
                                                    batch_size_text, original_language_text, target_languages_text],
                                            outputs=output_text_text) # Output only to text box


            with gr.TabItem("Folder Excel Translator"):
                with gr.Row():
                    with gr.Column():
                        input_folder_fexcel = gr.File(file_count="directory", label="ÈÄâÊã©ExcelÊñá‰ª∂ÊâÄÂú®Êñá‰ª∂Â§π")
                        with gr.Row():
                            start_row_fexcel = gr.Number(value=yaml_data["excel_config"]["default_start_row"], label="Ëµ∑ÂßãË°å")
                        with gr.Row():
                            row_selection_fexcel = gr.Radio(choices=["ÁâπÂÆöË°å", "ÊâÄÊúâË°å"], label="Ë°åÈÄâÊã©", value="ÁâπÂÆöË°å")
                            end_row_fexcel = gr.Number(value=yaml_data["excel_config"]["default_end_row"], label="ÁªàÊ≠¢Ë°å", visible=True)
                        row_selection_fexcel.change(update_row_selection, inputs=row_selection_fexcel, outputs=end_row_fexcel)
                        with gr.Row():
                            target_column_fexcel = gr.Textbox(value=yaml_data["excel_config"]["default_target_column"], label="ÁõÆÊ†áÂàó")
                            start_column_fexcel = gr.Textbox(value=yaml_data["excel_config"]["default_start_column"], label="ÁªìÊûúÂÜôÂÖ•Âàó")
                        with gr.Row():
                            selected_model_fexcel = gr.Dropdown(choices=list(available_models.keys()), label="ÈÄâÊã©Âü∫Ê®°Âûã", value=default_model_name)
                            selected_lora_model_fexcel = gr.Dropdown(choices=initial_lora_choices, label="ÈÄâÊã©LoraÊ®°Âûã", value='')
                            selected_gpu_fexcel = gr.Dropdown(choices=available_gpus, label="ÈÄâÊã©GPU", value=available_gpus[0] if available_gpus else None)
                            batch_size_fexcel = gr.Number(value=yaml_data.get("excel_config", {}).get("default_batch_size", 10), label="ÊâπÂ§ÑÁêÜÂ§ßÂ∞è", visible=True) # Use default from yaml or 10
                        with gr.Row():
                            original_language_fexcel = gr.Dropdown(choices=initial_original_choices, label="ÂéüÂßãËØ≠Ë®Ä", value=default_original_language)
                            target_languages_fexcel = gr.Dropdown(choices=initial_target_choices, label="ÁõÆÊ†áËØ≠Ë®Ä", multiselect=True, value=default_target_language_multi)
                        translate_button_fexcel = gr.Button("Translate Folder")
                    with gr.Column():
                        model_explanation_textbox_fexcel = gr.Textbox(label="Ê®°Âûã‰ªãÁªç", lines=5, value=initial_explanation)
                        output_text_fexcel = gr.Textbox(label="Â§ÑÁêÜÁä∂ÊÄÅ", lines=5)
                        output_folder_fexcel = gr.File(label="‰∏ãËΩΩÂ§ÑÁêÜÂêéÁöÑZipÊñá‰ª∂")

                selected_model_fexcel.change(update_choices,
                                            inputs=[selected_model_fexcel],
                                            outputs=[original_language_fexcel, target_languages_fexcel, selected_lora_model_fexcel, model_explanation_textbox_fexcel])
                translate_button_fexcel.click(translate_excel_folder,
                                            inputs=[input_folder_fexcel, start_row_fexcel, end_row_fexcel, start_column_fexcel, target_column_fexcel,
                                                    selected_model_fexcel, selected_lora_model_fexcel, selected_gpu_fexcel, batch_size_fexcel,
                                                    original_language_fexcel, target_languages_fexcel, row_selection_fexcel],
                                            outputs=[output_text_fexcel, output_folder_fexcel])


            with gr.TabItem("Folder Markdown & Docx Translator"):
                with gr.Row():
                    with gr.Column():
                         # Allow uploading directory or individual files
                        input_folder_mdoc = gr.File(file_count="multiple", file_types=['.md', '.docx', '.pptx', '.xlsx', '.xls'], label="ÈÄâÊã©Markdown, Docx, PPTXÊñá‰ª∂ÊàñÊñá‰ª∂Â§π") # Allow multiple file types
                        with gr.Row():
                            selected_model_mdoc = gr.Dropdown(choices=list(available_models.keys()), label="ÈÄâÊã©Âü∫Ê®°Âûã", value=default_model_name)
                            selected_lora_model_mdoc = gr.Dropdown(choices=initial_lora_choices, label="ÈÄâÊã©LoraÊ®°Âûã", value='')
                            selected_gpu_mdoc = gr.Dropdown(choices=available_gpus, label="ÈÄâÊã©GPU", value=available_gpus[0] if available_gpus else None)
                            batch_size_mdoc = gr.Number(value=5, label="ÊâπÂ§ÑÁêÜÂ§ßÂ∞è", visible=True) # Adjust default batch size as needed
                        with gr.Row():
                            original_language_mdoc = gr.Dropdown(choices=initial_original_choices, label="ÂéüÂßãËØ≠Ë®Ä", value=default_original_language)
                            # Changed target_language to single select for this tab as per original code structure
                            target_language_mdoc = gr.Dropdown(choices=initial_target_choices, label="ÁõÆÊ†áËØ≠Ë®Ä", value=default_target_language_single)
                        translate_button_mdoc = gr.Button("Translate Folder/Files")
                    with gr.Column():
                        model_explanation_textbox_mdoc = gr.Textbox(label="Ê®°Âûã‰ªãÁªç", lines=5, value=initial_explanation)
                        output_text_mdoc = gr.Textbox(label="Â§ÑÁêÜÁä∂ÊÄÅ", lines=5)
                        output_folder_mdoc = gr.File(label="‰∏ãËΩΩÂ§ÑÁêÜÂêéÁöÑZipÊñá‰ª∂")

                selected_model_mdoc.change(update_choices,
                                            inputs=[selected_model_mdoc],
                                            # Ensure target_language_mdoc is updated correctly (might need adjustment in update_choices return if it expects multiselect always)
                                            # Let's assume update_choices returns (orig_lang_dd, target_lang_multi_dd, lora_dd, explanation)
                                            # We map target_lang_multi_dd's choices to the single-select target_language_mdoc
                                            outputs=[original_language_mdoc, target_language_mdoc, selected_lora_model_mdoc, model_explanation_textbox_mdoc]) # Map 2nd output to single dropdown

                # Update click handler inputs/outputs
                translate_button_mdoc.click(translate_markdown_folder,
                                          inputs=[input_folder_mdoc, selected_model_mdoc, selected_lora_model_mdoc, selected_gpu_mdoc,
                                                  batch_size_mdoc, original_language_mdoc, target_language_mdoc], # Pass single target lang
                                          outputs=[output_text_mdoc, output_folder_mdoc])

            with gr.TabItem("33ËØ≠ÁøªËØë"):
                gr.Markdown("### ‰∏ÄÈîÆÂõ∫ÂÆöÂ§öËØ≠Ë®ÄÁøªËØë\nÊ≠§ÂäüËÉΩÂ∞ÜÊääÊÇ®‰∏ä‰º†ÁöÑExcelÊñá‰ª∂‰∏≠ **AÂàó** ÁöÑÊñáÊú¨Ôºà‰ªéÁ¨¨2Ë°åÂºÄÂßãÔºâÔºå‰ΩøÁî®ÊâÄÈÄâÊ®°ÂûãÔºåÁøªËØëÊàê‰ª£Á†Å‰∏≠È¢ÑËÆæÁöÑ30Â§öÁßçËØ≠Ë®ÄÔºåÂπ∂‰ªé **BÂàó** ÂºÄÂßã‰æùÊ¨°ÂÜôÂÖ•ÁªìÊûú„ÄÇ")
                with gr.Row():
                    with gr.Column(scale=2):
                        input_file_fixed = gr.File(label="‰∏ä‰º†ÂæÖÁøªËØëÁöÑExcelÊñá‰ª∂")
                        
                        translate_button_fixed = gr.Button("üöÄ ÂºÄÂßãÁøªËØë", variant="primary")
                    
                    with gr.Column(scale=1):
                        model_explanation_textbox_fixed = gr.Textbox(label="Ê®°Âûã‰ªãÁªç", lines=10, value=initial_explanation, interactive=False)
                        output_text_fixed = gr.Textbox(label="Â§ÑÁêÜÁä∂ÊÄÅ‰∏éÊó•Âøó", lines=10, interactive=False)
                        output_file_fixed = gr.File(label="‰∏ãËΩΩÁøªËØëÂêéÁöÑÊñá‰ª∂")
                
                translate_button_fixed.click(
                    translate_excel_fixed_languages,
                    inputs=[
                        input_file_fixed
                    ],
                    outputs=[output_text_fixed, output_file_fixed]
                )

            with gr.TabItem("ÊúØËØ≠Ë°®Ê†°È™å"):
                 with gr.Row():
                     with gr.Column():
                         input_folder_gloss = gr.File(file_count="directory", label="ÈÄâÊã©ÂåÖÂê´ÂæÖÊ†°È™åExcelÊñá‰ª∂ÁöÑÊñá‰ª∂Â§π")
                         with gr.Row():
                             row_selection_gloss = gr.Radio(choices=["ÁâπÂÆöË°å", "ÊâÄÊúâË°å"], label="Ë°åÈÄâÊã©", value="ÁâπÂÆöË°å")
                             start_row_gloss = gr.Number(value=yaml_data["excel_config"]["default_start_row"], label="Ëµ∑ÂßãË°å")
                             end_row_gloss = gr.Number(value=yaml_data["excel_config"]["default_end_row"], label="ÁªàÊ≠¢Ë°å", visible=True)
                         row_selection_gloss.change(update_row_selection, inputs=row_selection_gloss, outputs=end_row_gloss)
                         with gr.Row():
                              # Use defaults from YAML or hardcode if not present
                              default_orig_col = yaml_data.get("glossary_config", {}).get("original_column", "J")
                              default_ref_col = yaml_data.get("glossary_config", {}).get("reference_column", "G")
                              default_trans_col = yaml_data.get("glossary_config", {}).get("translated_column", "H")
                              default_remark_col = yaml_data.get("glossary_config", {}).get("remark_column", "I")
                              original_column_gloss = gr.Textbox(default_orig_col, label="ÂéüÊñáÂàó")
                              reference_column_gloss = gr.Textbox(default_ref_col, label="ÂèÇËÄÉÂàó")
                              translated_column_gloss = gr.Textbox(default_trans_col, label="Â∑≤ÁøªËØëÂàó")
                              remark_column_gloss = gr.Textbox(default_remark_col, label="Â§áÊ≥®ÂÜôÂÖ•Âàó")
                         glossary_check_button = gr.Button("ÂºÄÂßãÊ£ÄÊµã")

                     with gr.Column():
                         output_text_gloss = gr.Textbox(label="Ê£ÄÊµãÁªìÊûúÊëòË¶Å", lines=20, show_copy_button=True)
                         output_folder_gloss = gr.File(label="‰∏ãËΩΩÊ†áÊ≥®ÂêéÁöÑZipÊñá‰ª∂")

                 glossary_check_button.click(glossary_check,
                                            inputs=[input_folder_gloss, start_row_gloss, end_row_gloss, original_column_gloss, reference_column_gloss,
                                                    translated_column_gloss, row_selection_gloss, remark_column_gloss],
                                            outputs=[output_text_gloss, output_folder_gloss])

    return interface


main_ui = webui()

if __name__ == "__main__":
    # Consider adding server_name="0.0.0.0" to allow access from other devices on the network
    main_ui.launch(share=True, server_port=8080) # share=True generates a public link (requires internet)